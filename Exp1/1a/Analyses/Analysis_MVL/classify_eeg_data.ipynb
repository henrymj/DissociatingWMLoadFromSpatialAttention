{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as sista\n",
    "import scipy.io as sio\n",
    "import mat73 as mt\n",
    "import matplotlib.pyplot as plt\n",
    "#import mord\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#subjects to include\n",
    "subj_name = ['1','2','4','5','6','7','8','9','11','12']\n",
    "nsubj = len(subj_name)\n",
    "\n",
    "#number of electrodes\n",
    "ne = 30\n",
    "\n",
    "#trial timing\n",
    "t0 = -400\n",
    "te = 1551\n",
    "\n",
    "# Classification parameters and setup\n",
    "time_window = 50 #bin width\n",
    "time_step = 25 #bin offset (if smaller than tw, will have overlap)\n",
    "n_splits = 1000 #no of iterations\n",
    "trial_average = [20] #no of trials to average\n",
    "\n",
    "# ss: 2 vs 4; cond: same vs diff vs simultaneous\n",
    "group_label = ['condition','setSize'] #conditions to be referenced\n",
    "group_dict = {0:[2,2],1:[1,4]}\n",
    "\n",
    "cs = group_dict\n",
    "num_cs = len(group_dict)\n",
    "\n",
    "# create experiment, data wrangler, and classification objects \n",
    "samples = np.arange(t0,te,2)\n",
    "sample_step = samples[1]-samples[0]\n",
    "t = samples[0:samples.shape[0]-int(time_window/sample_step)+1:int(time_step/sample_step)]\n",
    "\n",
    "#train/test indices using stratified randomized folds\n",
    "#preserves the percentage of samples for each class\n",
    "cross_val = StratifiedShuffleSplit(n_splits,test_size=0.2)\n",
    "classifier = LogisticRegression()\n",
    "#z-score normalization\n",
    "scaler = StandardScaler()\n",
    "le = LabelEncoder()\n",
    "\n",
    "#load labels\n",
    "fn = cwd + '/allsubs_data.mat'\n",
    "beh = mt.loadmat(fn)\n",
    "\n",
    "#set up directories\n",
    "df = cwd + '/EEGData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef90db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preallocate\n",
    "acc = np.zeros((nsubj,np.size(t),n_splits))*np.nan\n",
    "acc_shuff = np.zeros((nsubj,np.size(t),n_splits))*np.nan\n",
    "conf_mat = np.zeros((nsubj,np.size(t),n_splits,num_cs,num_cs))*np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over subjects in experiment\n",
    "for isubj in range(nsubj):\n",
    "    print(isubj)\n",
    "\n",
    "    #filename\n",
    "    fn = df + subj_name[isubj] +'_EEG.mat'\n",
    "\n",
    "    subj_mat = mt.loadmat(fn,only_include = ['erp/arfDat/baselined','erp/arf/artifactIndCleaned'])\n",
    "    \n",
    "    #artifact rejection indices, use only clean trials\n",
    "    idx = subj_mat['erp']['arf']['artifactIndCleaned']==0\n",
    "    \n",
    "    #data, baselined ERP\n",
    "    erp_baselined = subj_mat['erp']['arfDat']['baselined']\n",
    "    erp_baselined = erp_baselined[idx,:ne,:]\n",
    "\n",
    "    #condition labels\n",
    "    labels = np.zeros((len(group_label),len(idx)))*np.nan\n",
    "    for icond,cond in enumerate(group_label):\n",
    "        labels[icond,:] = beh['allsubs'][group_label[icond]][isubj][:]\n",
    "\n",
    "    cnt = 0\n",
    "    #conditions of interest, specified above\n",
    "    values = list(group_dict.values())\n",
    "    #preallocate to save new labels that only includes conditions of interest\n",
    "    labels_new = np.zeros((len(idx)))*np.nan\n",
    "    for val in values:\n",
    "        #for this type of trial, preallocate combinations of variables\n",
    "        include = np.zeros((len(group_label),len(idx)))*np.nan\n",
    "        for icond, cond in enumerate(group_label):\n",
    "            include[icond,:] = np.isin(labels[icond,:],val[icond])\n",
    "        # include trial if all conditions are satisfied\n",
    "        labels_new[np.sum(include,axis=0)==len(group_label)] = cnt\n",
    "        cnt += 1\n",
    "        \n",
    "    labels = labels_new\n",
    "    labels = np.array(labels)\n",
    "    labels = labels[idx] \n",
    "\n",
    "    #only trials from conditions of interest\n",
    "    label_idx = ~np.isnan(labels)\n",
    "    xdata = erp_baselined[label_idx,:,:]\n",
    "    ydata = labels[label_idx]\n",
    "    \n",
    "    le.fit(ydata)\n",
    "    ydata = le.fit_transform(ydata)\n",
    "    \n",
    "    #balance the number for each class\n",
    "    unique_labels, counts_labels = np.unique(ydata, return_counts=True)\n",
    "    downsamp = min(counts_labels)\n",
    "    label_idx=[]\n",
    "    for label in unique_labels:\n",
    "        label_idx = np.append(label_idx,np.random.choice(np.arange(len(ydata))[ydata == label],downsamp,replace=False))\n",
    "    #include only selected trials\n",
    "    xdata = xdata[label_idx.astype(int)]\n",
    "    ydata = ydata[label_idx.astype(int)]\n",
    "\n",
    "    #why was balancing happening before this\n",
    "    #average trials (binning)\n",
    "    #already equating number of trials here?\n",
    "    unique_labels, counts_labels = np.unique(ydata, return_counts=True)\n",
    "    min_count = np.floor(min(counts_labels)/trial_average)*trial_average\n",
    "    nbin = int(min_count/trial_average) #how many mega-trials per condition\n",
    "    trial_groups = np.tile(np.arange(nbin),trial_average)\n",
    "    #mega_trials X electrodes X time\n",
    "    xdata_new = np.zeros((nbin*len(unique_labels),xdata.shape[1],xdata.shape[2]))\n",
    "    count = 0\n",
    "    #for each mega-trial of each condition\n",
    "    for ilabel in unique_labels:\n",
    "        for igroup in np.unique(trial_groups):\n",
    "            #find trials in this condition, grab the first min_count, average across their groups\n",
    "            #always choosing the first trials and cutting off the last\n",
    "            #same trials are getting averaged over??\n",
    "            xdata_new[count] = np.mean(xdata[ydata==ilabel][:int(min_count)][trial_groups==igroup],axis=0)\n",
    "            count += 1\n",
    "            \n",
    "    #run classification on averaged data        \n",
    "    ydata_new = np.repeat(unique_labels,nbin)\n",
    "    xdata,ydata = xdata_new,ydata_new\n",
    "\n",
    "    ifold = 0\n",
    "\n",
    "    #split data into train and test, training samples are not balanced but close\n",
    "    for train_index, test_index in cross_val.split(xdata[:,0,0],ydata):\n",
    "        \n",
    "        #randomizing which groups of averaged trials will serve as train and test\n",
    "        X_train_all, X_test_all = xdata[train_index], xdata[test_index]\n",
    "\n",
    "        #don't see an imbalance issue\n",
    "        y_train, y_test = ydata[train_index].astype(int), ydata[test_index].astype(int)\n",
    "        if sum(y_test==1)==sum(y_test==0)== False:\n",
    "            k\n",
    "\n",
    "        #shuffle labels for chance classification\n",
    "        y_test_shuffle = np.random.permutation(y_test)\n",
    "\n",
    "        #for each time point\n",
    "        for itime, time in enumerate(t):\n",
    "\n",
    "            time_window_idx = (samples >= time) & (samples <  time + time_window)\n",
    "\n",
    "            #data for this time bin\n",
    "            X_train = np.mean(X_train_all[...,time_window_idx],2)\n",
    "            X_test = np.mean(X_test_all[...,time_window_idx],2)\n",
    "\n",
    "            #z-score each electrode across trials at this time point\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            #do classification\n",
    "            classifier.fit(X_train, y_train)\n",
    "\n",
    "            acc[isubj,itime,ifold] = classifier.score(X_test,y_test)\n",
    "            acc_shuff[isubj,itime,ifold] = classifier.score(X_test,y_test_shuffle)\n",
    "            conf_mat[isubj,itime,ifold] = confusion_matrix(y_test,y_pred=classifier.predict(X_test))\n",
    "\n",
    "        ifold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f382331",
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('ss2d_ss4s_exp1.mat', mdict={'acc': acc, 'acc_shuff': acc_shuff,'conf_mat':conf_mat})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
